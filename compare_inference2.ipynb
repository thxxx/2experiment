{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d42203-ae86-4da4-b189-ee998fd135e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e628879-7ee1-4512-8053-3f48fc0d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator\n",
    "from config import Config\n",
    "from audiomodel import AudioProcessing\n",
    "from audiodataset import AudioDataset, TestDataset\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def build_model(cfg):\n",
    "        from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "        \"\"\"Instantiate models and optimizer.\"\"\"     \n",
    "        compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        lm = load_lm_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        return compression_model, lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572ff0f-c6e3-427a-b235-58351460ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"frog followed by woosh\",\n",
    "    \"rain, frog\",\n",
    "    \"fire, wood building falling\",\n",
    "    \"gun sound and then child crying\",\n",
    "    \"crying monkey\",\n",
    "    \"busy office, ambience\",\n",
    "    \"restaurant kitchen, ambience\",\n",
    "    \"big, church bell\",\n",
    "    \"cartoon, crying\",\n",
    "    \"morning alarm\",\n",
    "    \"machine exploding, parts falling\",\n",
    "    \"missile firing and exploding\",\n",
    "    \"whoosh, ice\",\n",
    "    \"dragon wings flapping\",\n",
    "    \"biting an apple\",\n",
    "    \"walking on the shallow water\",\n",
    "    \"engine starting up\",\n",
    "    \"violin, concert hall\",\n",
    "    \"rolling dice\",\n",
    "    \"wine glass falling\",\n",
    "    \"printer printing paper\",\n",
    "    \"laser gun\",\n",
    "    \"multimedia, notification\",\n",
    "    \"truck accelerating\",\n",
    "    \"frying chicken in a oil\",\n",
    "    \"running, basketball court\",\n",
    "    \"lightning hits tree\",\n",
    "    \"rifle reloading\",\n",
    "    \"fart sound machine gun\",\n",
    "    \"crickets chirping\",\n",
    "    \"chainsaw cutting tree\",\n",
    "    \"girl, whispering\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4de35-c5f4-4a18-8b06-4a71b251d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(prompts=[\"beep\"], n=5):\n",
    "    cfg = Config()\n",
    "    cfg.update(**{\"prompts\": [p for p in prompts for _ in range(3)]})\n",
    "    \n",
    "    accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "    save_path = \"./test22\"\n",
    "    make_dir(save_path)\n",
    "    cfg.update(**{\"save_path\": save_path})\n",
    "    \n",
    "    compression_model, lm = build_model(cfg)\n",
    "    model = AudioProcessing(cfg, lm)\n",
    "    \n",
    "    test_dataset = TestDataset(cfg)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "    \n",
    "    model, compression_model = accelerator.prepare(model, compression_model)\n",
    "    model_path = os.path.join(\"./compare/44.pth\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    model.eval()\n",
    "    compression_model.eval()\n",
    "    if accelerator.is_main_process:         \n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "        audio_num = 1\n",
    "        for test_step, batch in enumerate(test_dataloader):\n",
    "            gen_tokens, gen_audio = unwrapped_model.inference(batch, unwrapped_vae)\n",
    "            prompt = batch[0]\n",
    "            print(prompt)\n",
    "            audio_filename = f\"{prompt}_{audio_num}.wav\"\n",
    "            unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "            from IPython.display import Audio\n",
    "            display(Audio(data=gen_audio[0].detach().cpu().numpy(), rate=cfg.sample_rate))\n",
    "            audio_num += 1\n",
    "            if audio_num > 5:\n",
    "                audio_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21c962-274a-46e2-9e51-100375c7d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfg = Config()\n",
    "cfg.update(**{\"prompts\": [p for p in prompts for _ in range(3)]})\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "save_path = \"./test_mix\"\n",
    "make_dir(save_path)\n",
    "cfg.update(**{\"save_path\": save_path})\n",
    "\n",
    "compression_model, lm = build_model(cfg)\n",
    "model = AudioProcessing(cfg, lm)\n",
    "\n",
    "test_dataset = TestDataset(cfg)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "model, compression_model = accelerator.prepare(model, compression_model)\n",
    "model_path = os.path.join(\"./compare/base_19.pth\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "compression_model.eval()\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "    audio_num = 1\n",
    "    for test_step, batch in tqdm(enumerate(test_dataloader)):\n",
    "        gen_tokens, gen_audio = unwrapped_model.inference(batch, unwrapped_vae)\n",
    "        prompt = batch[0]\n",
    "        print(prompt)\n",
    "        audio_filename = f\"{prompt}_{audio_num}.wav\"\n",
    "        unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "        from IPython.display import Audio\n",
    "        # display(Audio(data=gen_audio[0].detach().cpu().numpy(), rate=cfg.sample_rate))\n",
    "        AudioSignal(gen_audio[0].detach().cpu().numpy(), sample_rate=cfg.sample_rate).widget()\n",
    "        audio_num += 1\n",
    "        if audio_num > 5:\n",
    "            audio_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5bffc-26ab-4c0a-a4ac-7e6d06657f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "del model\n",
    "del compression_model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfg = Config()\n",
    "cfg.update(**{\"prompts\": [p for p in prompts for _ in range(3)]})\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "save_path = \"./test_mix\"\n",
    "make_dir(save_path)\n",
    "cfg.update(**{\"save_path\": save_path})\n",
    "\n",
    "compression_model, lm = build_model(cfg)\n",
    "model = AudioProcessing(cfg, lm)\n",
    "\n",
    "test_dataset = TestDataset(cfg)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "model, compression_model = accelerator.prepare(model, compression_model)\n",
    "model_path = os.path.join(\"./output_dir_total22/19.pth\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "compression_model.eval()\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "    audio_num = 1\n",
    "    for test_step, batch in tqdm(enumerate(test_dataloader)):\n",
    "        gen_tokens, gen_audio = unwrapped_model.inference(batch, unwrapped_vae)\n",
    "        prompt = batch[0]\n",
    "        print(prompt)\n",
    "        audio_filename = f\"{prompt}_{audio_num}.wav\"\n",
    "        unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "        from IPython.display import Audio\n",
    "        # display(Audio(data=gen_audio[0].detach().cpu().numpy(), rate=cfg.sample_rate))\n",
    "        AudioSignal(gen_audio[0].detach().cpu().numpy(), sample_rate=cfg.sample_rate).widget()\n",
    "        audio_num += 1\n",
    "        if audio_num > 5:\n",
    "            audio_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa5da6-ea6c-487c-968b-da62e13667fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfg = Config()\n",
    "cfg.update(**{\"prompts\": [p for p in prompts for _ in range(3)]})\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "save_path = \"./test_mix\"\n",
    "make_dir(save_path)\n",
    "cfg.update(**{\"save_path\": save_path})\n",
    "\n",
    "compression_model, lm = build_model(cfg)\n",
    "model = AudioProcessing(cfg, lm)\n",
    "\n",
    "test_dataset = TestDataset(cfg)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "model, compression_model = accelerator.prepare(model, compression_model)\n",
    "model_path = os.path.join(\"./compare/base_19.pth\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "compression_model.eval()\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "    audio_num = 1\n",
    "    for test_step, batch in tqdm(enumerate(test_dataloader)):\n",
    "        gen_tokens, gen_audio = unwrapped_model.inference(batch, unwrapped_vae)\n",
    "        prompt = batch[0]\n",
    "        print(prompt)\n",
    "        audio_filename = f\"{prompt}_{audio_num}.wav\"\n",
    "        unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "        from IPython.display import Audio\n",
    "        # display(Audio(data=gen_audio[0].detach().cpu().numpy(), rate=cfg.sample_rate))\n",
    "        AudioSignal(gen_audio[0].detach().cpu().numpy(), sample_rate=cfg.sample_rate).widget()\n",
    "        audio_num += 1\n",
    "        if audio_num > 5:\n",
    "            audio_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc6107-75cb-4029-9b98-8c1c2fd34c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "del model\n",
    "del compression_model\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914f8d5-4566-407c-944a-85ae08e53629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "\n",
    "cfg = Config()\n",
    "cfg.update(**{\"prompts\": [p for p in prompts for _ in range(3)]})\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "save_path = \"./test_concat\"\n",
    "make_dir(save_path)\n",
    "cfg.update(**{\"save_path\": save_path})\n",
    "\n",
    "compression_model, lm = build_model(cfg)\n",
    "model = AudioProcessing(cfg, lm)\n",
    "\n",
    "test_dataset = TestDataset(cfg)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "model, compression_model = accelerator.prepare(model, compression_model)\n",
    "model_path = os.path.join(\"./compare/concat_20.pth\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "compression_model.eval()\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "    audio_num = 1\n",
    "    for test_step, batch in tqdm(enumerate(test_dataloader)):\n",
    "        gen_tokens, gen_audio = unwrapped_model.inference(batch, unwrapped_vae)\n",
    "        prompt = batch[0]\n",
    "        print(prompt)\n",
    "        audio_filename = f\"{prompt}_{audio_num}.wav\"\n",
    "        unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "        from IPython.display import Audio\n",
    "        # display(Audio(data=gen_audio[0].detach().cpu().numpy(), rate=cfg.sample_rate))\n",
    "        AudioSignal(gen_audio[0].detach().cpu().numpy(), sample_rate=cfg.sample_rate).widget()\n",
    "        audio_num += 1\n",
    "        if audio_num > 5:\n",
    "            audio_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356fe502-ba11-463b-a6eb-8504067b1363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
